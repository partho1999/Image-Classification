{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EffectnetB0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeJ5d76Fb49F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCNEDmmxb70j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ecf8d1-13f8-4653-82e7-4182b9e10312"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zZgRE2zelw4"
      },
      "source": [
        "#!unzip \"/content/drive/MyDrive/dataset/dataset/dataset.zip\" -d \"/content/drive/MyDrive\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuM8dWgkfDzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2dbde60-91cd-4be0-b87e-ab24174af44d"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "import h5py\n",
        "\n",
        "\n",
        "\n",
        "dataset_path = os.listdir('/content/drive/MyDrive/dataset')\n",
        "\n",
        "# print (dataset_path)  #what kinds of classes are in this dataset\n",
        "#\n",
        "# print(\"Types of classes labels found: \", len(dataset_path))\n",
        "\n",
        "class_labels = []\n",
        "\n",
        "for item in dataset_path:\n",
        " # Get all the file names\n",
        " all_classes = os.listdir('/content/drive/MyDrive/dataset' + '/' +item)\n",
        " #print(all_classes)\n",
        "\n",
        " # Add them to the list\n",
        " for room in all_classes:\n",
        "    class_labels.append((item, str('dataset_path' + '/' +item) + '/' + room))\n",
        "    #print(class_labels[:5])\n",
        "\n",
        "df = pd.DataFrame(data=class_labels, columns=['Labels', 'image'])\n",
        "#print(df.head())\n",
        "#print(df.tail())\n",
        "\n",
        "# Let's check how many samples for each category are present\n",
        "#print(\"Total number of images in the dataset: \", len(df))\n",
        "\n",
        "label_count = df['Labels'].value_counts()\n",
        "#print(label_count)\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/dataset/'\n",
        "#dataset_path = os.listdir('dataset')\n",
        "\n",
        "im_size = 224\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for i in dataset_path:\n",
        "    data_path = path + str(i)\n",
        "    filenames = [i for i in os.listdir(data_path)]\n",
        "\n",
        "    for f in filenames:\n",
        "        img = cv2.imread(data_path + '/' + f)\n",
        "        img = cv2.resize(img, (im_size, im_size))\n",
        "        images.append(img)\n",
        "        labels.append(i)\n",
        "\n",
        "#This model takes input images of shape (224, 224, 3), and the input data should range [0, 255].\n",
        "\n",
        "images = np.array(images)\n",
        "\n",
        "images = images.astype('float32') / 255.0\n",
        "images.shape\n",
        "\n",
        "#preprocessing and Encoding\n",
        "y=df['Labels'].values\n",
        "#print(y)\n",
        "\n",
        "y_labelencoder = LabelEncoder ()\n",
        "y = y_labelencoder.fit_transform (y)\n",
        "#print (y)\n",
        "\n",
        "y=y.reshape(-1,1)\n",
        "\n",
        "#ColumnTransformer\n",
        "ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "Y = ct.fit_transform(y) #.toarray()\n",
        "# print(Y[:5])\n",
        "# print(Y[35:])\n",
        "\n",
        "#Shuffle\n",
        "#train_test_split\n",
        "\n",
        "\n",
        "images, Y = shuffle(images, Y, random_state=1)\n",
        "\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.05, random_state=415)\n",
        "\n",
        "#inpect the shape of the training and testing.\n",
        "# print(train_x.shape)\n",
        "# print(train_y.shape)\n",
        "# print(test_x.shape)\n",
        "# print(test_y.shape)\n",
        "\n",
        "#model\n",
        "NUM_CLASSES = 2\n",
        "IMG_SIZE = 224\n",
        "size = (IMG_SIZE, IMG_SIZE)\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "\n",
        "# Using model without transfer learning\n",
        "\n",
        "outputs = EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES)(inputs)\n",
        "\n",
        "\n",
        "#Model Fiting\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"] )\n",
        "\n",
        "# checkpoint_filepath = 'E:\\codes\\EfficentNetB0\\Image-Classification-Using-EfficientNets\\my_model'\n",
        "#\n",
        "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=checkpoint_filepath,\n",
        "#     save_weights_only=True,\n",
        "#     monitor='accuracy',\n",
        "#     mode='max',\n",
        "#     save_best_only=True)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model.fit(train_x, train_y, epochs=20, callbacks=[cp_callback], verbose=2)\n",
        "\n",
        "model.save('n_model.h5')\n",
        "\n",
        "\n",
        "#model.models.save_model(\"my_model.h5\")\n",
        "#model.save(\"my_model\")\n",
        "# tf.keras.models.save_model(\n",
        "#     model,r\"E:\\codes\\EfficentNetB0\\Image-Classification-Using-EfficientNets\\my_model\", overwrite=True, include_optimizer=True, save_format='h5py',\n",
        "#     signatures=None, options=None, save_traces=True\n",
        "# )\n",
        "\n",
        "# serialize model to JSON\n",
        "\n",
        "# new_model =tf.keras.models.load_model('my_model.h5')\n",
        "#\n",
        "# loss, acc = new_model.evaluate(test_x, test_y)\n",
        "# print(\"accuracy:{:5.2f}%\".format(100*acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnetb0 (Functional)  (None, 2)                 4052133   \n",
            "=================================================================\n",
            "Total params: 4,052,133\n",
            "Trainable params: 4,010,110\n",
            "Non-trainable params: 42,023\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "137/137 - 113s - loss: 1.8745 - accuracy: 0.5432\n",
            "\n",
            "Epoch 00001: saving model to training_1/cp.ckpt\n",
            "Epoch 2/20\n",
            "137/137 - 70s - loss: 0.9383 - accuracy: 0.6490\n",
            "\n",
            "Epoch 00002: saving model to training_1/cp.ckpt\n",
            "Epoch 3/20\n",
            "137/137 - 70s - loss: 0.8125 - accuracy: 0.7357\n",
            "\n",
            "Epoch 00003: saving model to training_1/cp.ckpt\n",
            "Epoch 4/20\n",
            "137/137 - 70s - loss: 0.4479 - accuracy: 0.8435\n",
            "\n",
            "Epoch 00004: saving model to training_1/cp.ckpt\n",
            "Epoch 5/20\n",
            "137/137 - 70s - loss: 0.3058 - accuracy: 0.9082\n",
            "\n",
            "Epoch 00005: saving model to training_1/cp.ckpt\n",
            "Epoch 6/20\n",
            "137/137 - 70s - loss: 0.2047 - accuracy: 0.9492\n",
            "\n",
            "Epoch 00006: saving model to training_1/cp.ckpt\n",
            "Epoch 7/20\n",
            "137/137 - 70s - loss: 0.1400 - accuracy: 0.9561\n",
            "\n",
            "Epoch 00007: saving model to training_1/cp.ckpt\n",
            "Epoch 8/20\n",
            "137/137 - 70s - loss: 0.0979 - accuracy: 0.9700\n",
            "\n",
            "Epoch 00008: saving model to training_1/cp.ckpt\n",
            "Epoch 9/20\n",
            "137/137 - 70s - loss: 0.1145 - accuracy: 0.9703\n",
            "\n",
            "Epoch 00009: saving model to training_1/cp.ckpt\n",
            "Epoch 10/20\n",
            "137/137 - 70s - loss: 0.0928 - accuracy: 0.9757\n",
            "\n",
            "Epoch 00010: saving model to training_1/cp.ckpt\n",
            "Epoch 11/20\n",
            "137/137 - 70s - loss: 0.0756 - accuracy: 0.9794\n",
            "\n",
            "Epoch 00011: saving model to training_1/cp.ckpt\n",
            "Epoch 12/20\n",
            "137/137 - 70s - loss: 0.0989 - accuracy: 0.9822\n",
            "\n",
            "Epoch 00012: saving model to training_1/cp.ckpt\n",
            "Epoch 13/20\n",
            "137/137 - 70s - loss: 0.0765 - accuracy: 0.9833\n",
            "\n",
            "Epoch 00013: saving model to training_1/cp.ckpt\n",
            "Epoch 14/20\n",
            "137/137 - 70s - loss: 0.1045 - accuracy: 0.9824\n",
            "\n",
            "Epoch 00014: saving model to training_1/cp.ckpt\n",
            "Epoch 15/20\n",
            "137/137 - 70s - loss: 0.0750 - accuracy: 0.9842\n",
            "\n",
            "Epoch 00015: saving model to training_1/cp.ckpt\n",
            "Epoch 16/20\n",
            "137/137 - 70s - loss: 0.0725 - accuracy: 0.9776\n",
            "\n",
            "Epoch 00016: saving model to training_1/cp.ckpt\n",
            "Epoch 17/20\n",
            "137/137 - 70s - loss: 0.0341 - accuracy: 0.9888\n",
            "\n",
            "Epoch 00017: saving model to training_1/cp.ckpt\n",
            "Epoch 18/20\n",
            "137/137 - 70s - loss: 0.0403 - accuracy: 0.9895\n",
            "\n",
            "Epoch 00018: saving model to training_1/cp.ckpt\n",
            "Epoch 19/20\n",
            "137/137 - 70s - loss: 0.0410 - accuracy: 0.9904\n",
            "\n",
            "Epoch 00019: saving model to training_1/cp.ckpt\n",
            "Epoch 20/20\n",
            "137/137 - 70s - loss: 0.0273 - accuracy: 0.9918\n",
            "\n",
            "Epoch 00020: saving model to training_1/cp.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwXGmdodBZJO",
        "outputId": "ff438daf-1d90-4648-bd6c-5d2df3256597"
      },
      "source": [
        "from matplotlib.pyplot import imread\n",
        "from matplotlib.pyplot import imshow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "loaded_model = load_model('/content/n_model.h5')\n",
        "\n",
        "\n",
        "img_path = '/content/brain_tumor.jfif'\n",
        "\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "#x = img.img_to_array(img)\n",
        "x = np.array(img)\n",
        "\n",
        "#img = cv2.imread(img_path)\n",
        "#img = cv2.resize(img, (224, 224))\n",
        "\n",
        "x = np.expand_dims(img, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "print('Input image shape:', x.shape)\n",
        "\n",
        "my_image = imread(img_path)\n",
        "#cv2.imshow(my_image)\n",
        "\n",
        "\n",
        "preds= loaded_model.predict(x)\n",
        "print(preds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image shape: (1, 224, 224, 3)\n",
            "[[1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA-X0dHtBzin"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvjoUFeMBLIe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}